{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quora Question Pair Siamese Network.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMAQnb81HfNVyEAeyKULlEc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DC9IHK9XHyG4"},"source":["from time import time\n","import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import itertools\n","import datetime\n","import tensorflow as tf\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, Embedding, LSTM, Lambda\n","import keras.backend as K\n","from keras.optimizers import Adadelta\n","from keras.callbacks import ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e1s97CDH7LS","executionInfo":{"status":"ok","timestamp":1606997687461,"user_tz":-420,"elapsed":24962,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"ec459189-0b2f-4b8b-c113-7c23c3d833ab"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SrlpjTwtHGq6"},"source":["from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zx_mfgde5h4F","executionInfo":{"status":"ok","timestamp":1607004595895,"user_tz":-420,"elapsed":1082,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"7c103db5-3e83-4856-ff39-1c9c089496b1"},"source":["import string\n","import nltk\n","nltk.download('punkt')\n","import re\n","\n","def preprocess_question(df, raw):\n","  # string cleaning function\n","  def clean_word(word):\n","    word = str(word)\n","    # lowercase\n","    word = word.lower()\n","    word = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", word)\n","    # cleaning operators\n","    word = re.sub(r\"\\/\", \" \", word)\n","    word = re.sub(r\"\\^\", \" ^ \", word)\n","    word = re.sub(r\"\\+\", \" + \", word)\n","    word = re.sub(r\"\\-\", \"\", word)\n","    word = re.sub(r\"\\=\", \" = \", word)\n","    # translating abbreviations\n","    word = re.sub(r\"\\'re\", 'are', word)\n","    word = re.sub(r\"\\'ve\", 'have', word)\n","    word = re.sub(r\"\\'t\", 'not', word)\n","    word = re.sub(r\"\\'d\", 'would', word)\n","    word = re.sub(r\"\\'m\", 'am', word)\n","    word = re.sub(\"where's\", 'where is', word)\n","    word = re.sub(\"what's\", 'what is', word)\n","    word = re.sub(\"how's\", 'how is', word)\n","    word = re.sub(\"who's\", 'who is', word)\n","    word = re.sub(\"why's\", 'why is', word)\n","    word = re.sub(\"he's\", 'he is', word)\n","    word = re.sub(\"it's\", 'it is', word)\n","    word = re.sub(\"she's\", 'she is', word)\n","    word = re.sub(\"\\'s\", 'possession', word)\n","    word = re.sub(\"eg\", 'example', word)\n","    word = re.sub(\"usa\", 'america', word)\n","    word = re.sub(\"u.s.a\", 'america', word)\n","    word = re.sub(\"\\'ll\", 'will', word)\n","    word = re.sub(\"uk\", 'england', word)\n","    word = re.sub(\"911\", 'emergency', word)\n","    return word\n","\n","\n","  # removing punctuation function\n","  punct =[]\n","  punct += list(string.punctuation)\n","  punct += '’'\n","  punct.remove(\"'\")\n","  def remove_punctuations(text):\n","      for punctuation in punct:\n","          text = text.replace(punctuation, ' ')\n","      return text\n","\n","  # tokenization function\n","  from nltk import word_tokenize\n","\n","  df_input = df\n","  df_input['cleaned'] = df_input[raw].apply(clean_word)\n","  # getting rid of whitespaces\n","  df_input['cleaned'] = df_input[raw].apply(lambda x: str(x).replace('\\n', ' '))\n","  # remove links\n","  df_input['cleaned'] = df_input['cleaned'].str.replace('http\\S+|www.\\S+', '', case=False)\n","  # removing '>'\n","  df_input['cleaned'] = df_input['cleaned'].apply(lambda x: x.replace('&gt;', ''))\n","  # removing '<'\n","  df_input['cleaned'] = df_input['cleaned'].apply(lambda x: x.replace('&lt;', ''))\n","  # remove punctuation\n","  df_input['cleaned'] = df_input['cleaned'].apply(remove_punctuations)\n","  # remove ' s ' that was created after removing punctuations\n","  df_input['cleaned'] = df_input['cleaned'].apply(lambda x: str(x).replace(\" s \", \" \"))\n","  # apply tokenization\n","  df_input['tokenized'] = df_input['cleaned'].apply(word_tokenize)\n","  # remove stop words\n","  df_input['tokenized'] = df_input['tokenized'].apply(lambda word_list: [x for x in word_list if x not in ENGLISH_STOP_WORDS])\n","  # remove numbers\n","  df_input['tokenized'] = df_input['tokenized'].apply(lambda list_data: [x for x in list_data if x.isalpha()])\n","  # finalizing the preprocessing\n","  finale = list()\n","  for list_of_words in df_input.tokenized:\n","    finale.append(' '.join(list_of_words))\n","\n","  df_input['final_text'] = finale\n","\n","  return df_input.tokenized"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TL7CB_ZcXUS9"},"source":["quora = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Quora Question Pairs/train.csv')\n","questions_df = quora[['question1', 'question2', 'is_duplicate']]\n","y = questions_df['is_duplicate']\n","q1 = questions_df[['question1']]\n","q2 = questions_df[['question2']]\n","\n","# preprocess question1\n","question1_word_tokenized = preprocess_question(q1, 'question1')\n","question2_word_tokenized = preprocess_question(q2, 'question2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orLepPV5ZPxY"},"source":["q1_v = list()\n","q2_v = list()\n","vocabulary = dict()\n","inverse_vocabulary = ['<placeholder>']\n","for row_q1, row_q2 in zip(question1_word_tokenized, question2_word_tokenized):\n","  q1_number = []\n","  q2_number = []\n","  for word1, word2 in zip(row_q1, row_q2):\n","    if word1 not in vocabulary:\n","      vocabulary[word1] = len(inverse_vocabulary)\n","      q1_number.append(len(inverse_vocabulary))\n","      inverse_vocabulary.append(word1)\n","    else:\n","      q1_number.append(vocabulary[word1])\n","    \n","    if word2 not in vocabulary:\n","      vocabulary[word2] = len(inverse_vocabulary)\n","      q2_number.append(len(inverse_vocabulary))\n","      inverse_vocabulary.append(word2)\n","    else:\n","      q2_number.append(vocabulary[word2])\n","  q1_v.append(q1_number)\n","  q2_v.append(q2_number)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"Cnlvr28DfdYh","executionInfo":{"status":"ok","timestamp":1607004709375,"user_tz":-420,"elapsed":41393,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"9f0368e3-306c-4c4f-b0b3-c9199bc17543"},"source":["pd.DataFrame(zip(q1_v, q2_v))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[1, 2, 2, 3, 4, 5, 6]</td>\n","      <td>[1, 2, 2, 3, 4, 5, 6]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[1, 7, 9, 11, 13, 15]</td>\n","      <td>[1, 8, 10, 12, 14, 9]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[16, 17, 19, 20, 22, 24]</td>\n","      <td>[16, 18, 20, 21, 23, 25]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[26, 17, 29, 31, 16]</td>\n","      <td>[27, 28, 30, 30, 32]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[33, 34, 36, 38, 40]</td>\n","      <td>[33, 35, 37, 39, 36]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>404285</th>\n","      <td>[16, 3116, 76526, 379, 382, 3123, 3124]</td>\n","      <td>[16, 3116, 74640, 3121, 3122, 3123, 3124]</td>\n","    </tr>\n","    <tr>\n","      <th>404286</th>\n","      <td>[327, 2022, 443, 1474]</td>\n","      <td>[263, 958, 443, 1474]</td>\n","    </tr>\n","    <tr>\n","      <th>404287</th>\n","      <td>[1, 10376]</td>\n","      <td>[1, 10376]</td>\n","    </tr>\n","    <tr>\n","      <th>404288</th>\n","      <td>[1, 21907, 12041, 397, 2316, 2650, 57377, 8186...</td>\n","      <td>[17, 837, 2421, 2398, 2194, 17, 120, 63, 730, ...</td>\n","    </tr>\n","    <tr>\n","      <th>404289</th>\n","      <td>[1, 187, 2125, 6587]</td>\n","      <td>[1, 187, 2125, 6587]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>404290 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                        0                                                  1\n","0                                   [1, 2, 2, 3, 4, 5, 6]                              [1, 2, 2, 3, 4, 5, 6]\n","1                                   [1, 7, 9, 11, 13, 15]                              [1, 8, 10, 12, 14, 9]\n","2                                [16, 17, 19, 20, 22, 24]                           [16, 18, 20, 21, 23, 25]\n","3                                    [26, 17, 29, 31, 16]                               [27, 28, 30, 30, 32]\n","4                                    [33, 34, 36, 38, 40]                               [33, 35, 37, 39, 36]\n","...                                                   ...                                                ...\n","404285            [16, 3116, 76526, 379, 382, 3123, 3124]          [16, 3116, 74640, 3121, 3122, 3123, 3124]\n","404286                             [327, 2022, 443, 1474]                              [263, 958, 443, 1474]\n","404287                                         [1, 10376]                                         [1, 10376]\n","404288  [1, 21907, 12041, 397, 2316, 2650, 57377, 8186...  [17, 837, 2421, 2398, 2194, 17, 120, 63, 730, ...\n","404289                               [1, 187, 2125, 6587]                               [1, 187, 2125, 6587]\n","\n","[404290 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Ir6Uz-9fhhd","executionInfo":{"status":"ok","timestamp":1607008939123,"user_tz":-420,"elapsed":176692,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"c82d2940-fd9d-43a2-bd6a-7a55588cd768"},"source":["import gensim.downloader as api\n","model = api.load(\"glove-wiki-gigaword-300\")\n","embedding_dim = 300"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CZlevrvBpfRG"},"source":["embeddings = 1 * np.random.randn(len(vocabulary) + 1, 300)  \n","embeddings[0] = 0\n","\n","# Build the embedding matrix\n","for word, index in vocabulary.items():\n","    if word in model.vocab:\n","        embeddings[index] = model.word_vec(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEIp4qKn0AhU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"Ok4n-qyVqp_m","executionInfo":{"status":"ok","timestamp":1607008959183,"user_tz":-420,"elapsed":1156,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"bbdcd168-669c-442b-cbc0-f13f46a97863"},"source":["x = pd.DataFrame(zip(q1_v,q2_v))\n","x = x.rename({0:'q1', 1:'q2'}, axis = 'columns')\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>q1</th>\n","      <th>q2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[1, 2, 2, 3, 4, 5, 6]</td>\n","      <td>[1, 2, 2, 3, 4, 5, 6]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[1, 7, 9, 11, 13, 15]</td>\n","      <td>[1, 8, 10, 12, 14, 9]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[16, 17, 19, 20, 22, 24]</td>\n","      <td>[16, 18, 20, 21, 23, 25]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[26, 17, 29, 31, 16]</td>\n","      <td>[27, 28, 30, 30, 32]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[33, 34, 36, 38, 40]</td>\n","      <td>[33, 35, 37, 39, 36]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>404285</th>\n","      <td>[16, 3116, 76526, 379, 382, 3123, 3124]</td>\n","      <td>[16, 3116, 74640, 3121, 3122, 3123, 3124]</td>\n","    </tr>\n","    <tr>\n","      <th>404286</th>\n","      <td>[327, 2022, 443, 1474]</td>\n","      <td>[263, 958, 443, 1474]</td>\n","    </tr>\n","    <tr>\n","      <th>404287</th>\n","      <td>[1, 10376]</td>\n","      <td>[1, 10376]</td>\n","    </tr>\n","    <tr>\n","      <th>404288</th>\n","      <td>[1, 21907, 12041, 397, 2316, 2650, 57377, 8186...</td>\n","      <td>[17, 837, 2421, 2398, 2194, 17, 120, 63, 730, ...</td>\n","    </tr>\n","    <tr>\n","      <th>404289</th>\n","      <td>[1, 187, 2125, 6587]</td>\n","      <td>[1, 187, 2125, 6587]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>404290 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                       q1                                                 q2\n","0                                   [1, 2, 2, 3, 4, 5, 6]                              [1, 2, 2, 3, 4, 5, 6]\n","1                                   [1, 7, 9, 11, 13, 15]                              [1, 8, 10, 12, 14, 9]\n","2                                [16, 17, 19, 20, 22, 24]                           [16, 18, 20, 21, 23, 25]\n","3                                    [26, 17, 29, 31, 16]                               [27, 28, 30, 30, 32]\n","4                                    [33, 34, 36, 38, 40]                               [33, 35, 37, 39, 36]\n","...                                                   ...                                                ...\n","404285            [16, 3116, 76526, 379, 382, 3123, 3124]          [16, 3116, 74640, 3121, 3122, 3123, 3124]\n","404286                             [327, 2022, 443, 1474]                              [263, 958, 443, 1474]\n","404287                                         [1, 10376]                                         [1, 10376]\n","404288  [1, 21907, 12041, 397, 2316, 2650, 57377, 8186...  [17, 837, 2421, 2398, 2194, 17, 120, 63, 730, ...\n","404289                               [1, 187, 2125, 6587]                               [1, 187, 2125, 6587]\n","\n","[404290 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-x2bB3-4qp4o","executionInfo":{"status":"ok","timestamp":1607008981062,"user_tz":-420,"elapsed":4921,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"36741a41-489b-4a7d-a069-9fbeee612e43"},"source":["# finding out maximum length of question\n","max_length = max(x.q1.map(lambda x : len(x)).max(),\n","                 x.q2.map(lambda x : len(x)).max())\n","\n","# splitting data to train test split\n","test_size = 40000\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size)\n","\n","# creating q1 and q2 partition\n","x_train = {'q1': x_train.q1, 'q2':x_train.q2}\n","x_test = {'q1':x_test.q1, 'q2':x_test.q2}\n","\n","# convert y to numpy array\n","y_train = y_train.values\n","y_test = y_test.values\n","\n","# zero padding\n","for data, part in itertools.product([x_train, x_test], ['q1', 'q2']):\n","  data[part] = tf.keras.preprocessing.sequence.pad_sequences(data[part], maxlen=max_length)\n","\n","assert x_train['q1'].shape == x_train['q2'].shape\n","\n","if len(x_train['q1']) == len(y_train):\n","  print('Data preparation complete. We can begin training now.')\n","else:\n","  print('Train data and label has different length.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data preparation complete. We can begin training now.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VBnrryhSyrZU"},"source":["import tensorflow.keras.backend as k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNYQOgPNCYTD","executionInfo":{"status":"ok","timestamp":1607006269284,"user_tz":-420,"elapsed":5722,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"9402a118-6d96-481c-cae6-b639ed73f2ad"},"source":["!pip install keras_self_attention"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras_self_attention\n","  Downloading https://files.pythonhosted.org/packages/39/0d/b8ab8469ae55cea199574f4d2c30da4656d310a833a67bb422ad8a056bf0/keras-self-attention-0.47.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_self_attention) (1.18.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_self_attention) (2.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_self_attention) (2.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras_self_attention) (1.15.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.47.0-cp36-none-any.whl size=17289 sha256=78c936d95acda298be8dbaaaba68127b1ae413c8d143124e214c7fa4778f0da3\n","  Stored in directory: /root/.cache/pip/wheels/70/87/01/76c703d5401b65e323927c1fdc665f3fb143282ff67d71e859\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.47.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUCj-Qafr_RC","executionInfo":{"status":"ok","timestamp":1607014411363,"user_tz":-420,"elapsed":1066240,"user":{"displayName":"Grady Oktavian","photoUrl":"","userId":"04544528981743912888"}},"outputId":"aeebecf0-fd18-4e16-d7a0-a48d8fab993d"},"source":["# Model variables\n","k.clear_session()\n","batch_size = 128\n","n_epoch = 10\n","\n","q1_input = tf.keras.layers.Input(shape = (max_length, ), dtype = 'int32')\n","q2_input = tf.keras.layers.Input(shape = (max_length, ), dtype = 'int32')\n","\n","embedding_layer = tf.keras.layers.Embedding(len(embeddings), embedding_dim, \n","                                            weights = [embeddings], \n","                                            input_length = max_length,\n","                                            trainable = False)\n","\n","q1_embed = embedding_layer(q1_input)\n","q2_embed = embedding_layer(q2_input)\n","\n","lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences = True))\n","\n","q1_lstm = lstm_layer(q1_embed)\n","q2_lstm = lstm_layer(q2_embed)\n","\n","dot_layer = tf.keras.layers.dot([q1_lstm,q2_lstm], [1,1], normalize = True)\n","flatten_layer = tf.keras.layers.Flatten()(dot_layer)\n","\n","bn_layer_1 = tf.keras.layers.BatchNormalization()(flatten_layer)\n","dn_layer_1 = tf.keras.layers.Dense(256, activation = 'selu')(bn_layer_1)\n","do_layer_1 = tf.keras.layers.Dropout(0.3)(dn_layer_1)\n","\n","bn_layer_2 = tf.keras.layers.BatchNormalization()(do_layer_1)\n","dn_layer_2 = tf.keras.layers.Dense(256, activation = 'selu')(bn_layer_2)\n","do_layer_2 = tf.keras.layers.Dropout(0.3)(dn_layer_2)\n","\n","bn_layer_3 = tf.keras.layers.BatchNormalization()(do_layer_2)\n","dn_layer_3 = tf.keras.layers.Dense(256, activation = 'selu')(bn_layer_3)\n","do_layer_3 = tf.keras.layers.Dropout(0.3)(dn_layer_3)\n","\n","\n","#distance_layer = tf.keras.layers.Lambda(function = lambda x: manhattan_distance(x[0],x[1]), \n","#                                       output_shape = lambda x: x[0], name = 'distance')([q1_lstm, q2_lstm])\n","\n","dense_predict = tf.keras.layers.Dense(1, activation = 'sigmoid')(do_layer_3)\n","full_model = tf.keras.models.Model([q1_input, q2_input], dense_predict)\n","\n","full_model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'adam', metrics = ['accuracy'])\n","history = full_model.fit([x_train['q1'], x_train['q2']], y_train, batch_size=batch_size, epochs = n_epoch,\n","                         validation_data = ([x_test['q1'], x_test['q2']], y_test))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","2847/2847 [==============================] - 105s 37ms/step - loss: 0.5068 - accuracy: 0.7443 - val_loss: 0.4646 - val_accuracy: 0.7658\n","Epoch 2/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.4192 - accuracy: 0.7954 - val_loss: 0.4325 - val_accuracy: 0.7908\n","Epoch 3/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.3483 - accuracy: 0.8364 - val_loss: 0.4321 - val_accuracy: 0.7995\n","Epoch 4/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.2725 - accuracy: 0.8764 - val_loss: 0.4854 - val_accuracy: 0.7981\n","Epoch 5/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.2137 - accuracy: 0.9064 - val_loss: 0.5002 - val_accuracy: 0.8005\n","Epoch 6/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.1742 - accuracy: 0.9251 - val_loss: 0.4859 - val_accuracy: 0.8027\n","Epoch 7/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.1502 - accuracy: 0.9370 - val_loss: 0.5141 - val_accuracy: 0.8073\n","Epoch 8/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.1367 - accuracy: 0.9434 - val_loss: 0.6376 - val_accuracy: 0.8055\n","Epoch 9/10\n","2847/2847 [==============================] - 106s 37ms/step - loss: 0.1188 - accuracy: 0.9510 - val_loss: 0.6156 - val_accuracy: 0.8052\n","Epoch 10/10\n","2847/2847 [==============================] - 105s 37ms/step - loss: 0.1137 - accuracy: 0.9536 - val_loss: 0.6458 - val_accuracy: 0.8054\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jkbTi1hSr2Dy"},"source":["full_model.save_weights('siamese_full_model_weights.h5')"],"execution_count":null,"outputs":[]}]}